{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a SCCNN\n",
    "\n",
    "In this notebook, we will create and train a convolutional neural network in the simplicial complex domain, as proposed in the paper by [Yang et. al : Convolutional Learning on Simplicial Complexes (2023)](https://arxiv.org/abs/2301.11163). \n",
    "\n",
    "We train the model to perform complex classification using the shrec16 benchmark dataset. \n",
    "\n",
    "## Simplicial Complex Convolutional Neural Networks <a href=\"https://arxiv.org/pdf/2301.11163.pdf\">[SCCNN]</a>\n",
    "\n",
    "SCCNN extends the SCNN to the complex domain by accounting for inter-simplicial connections, i.e., contributions from simplices of adjacent orders. \n",
    "\n",
    "For example, we consider SCCNN layers in an SC of order two. At layer $t$, given the inputs on nodes, edges and faces, $\\mathbf{h}_{t}^0,\\mathbf{h}_{t}^1$ and $\\mathbf{h}_{t}^2$, the SCCNN layer contains the following\n",
    "$$\n",
    "    \\mathbf{h}_{t+1}^1 = \\sigma \\bigg[ \\mathbf{F}_{t,\\downarrow} \\mathbf{B}_{1}^\\top \\mathbf{h}_{t}^{0} + \\mathbf{F}_{t} \\mathbf{h}_t^1 + \\mathbf{F}_{t,\\uparrow} \\mathbf{B}_{2}  \\mathbf{h}_t^{2} \\bigg] \n",
    "$$\n",
    "where $\\mathbf{F}_t$ is the simplicial convolutional filter defined in the edge space, and $\\mathbf{F}_{t,\\downarrow}$ and $\\mathbf{F}_{t,\\uparrow}$ are the convolutional filters based on, respectively, only the lower and upper Laplacians. They are given by \n",
    "$$\n",
    "    \\mathbf{F}_{t} = {\\theta}_t + \\sum_{p_d=1}^{P_d} {\\theta}_{t,p_d} (\\mathbf{L}_{\\downarrow,1})^{p_d}  + \\sum_{p_u=1}^{P_u} {\\theta}_{t,p_u}  (\\mathbf{L}_{\\uparrow,1})^{p_u} \n",
    "$$\n",
    "$$\n",
    "    \\mathbf{F}_{t,\\downarrow} = {\\theta}_t + \\sum_{p_d=1}^{P_d} {\\theta}_{t,p_d} (\\mathbf{L}_{\\downarrow,1})^{p_d}  \n",
    "    \\text{ and } \n",
    "    \\mathbf{F}_{t,\\uparrow} = {\\theta}_t + \\sum_{p_u=1}^{P_u} {\\theta}_{t,p_u}  (\\mathbf{L}_{\\uparrow,1})^{p_u} \n",
    "$$\n",
    "\n",
    "Likewise, for the node output, we have \n",
    "$$\n",
    "    \\mathbf{h}_{t+1}^0 = \\sigma \\bigg[ \\mathbf{F}_{t} \\mathbf{h}_t^0 + \\mathbf{F}_{t,\\uparrow} \\mathbf{B}_{1}  \\mathbf{h}_t^{1} \\bigg]\n",
    "$$\n",
    "where $\\mathbf{F}_t$ and $\\mathbf{F}_{t,\\uparrow}$ are two graph filters essentially. \n",
    "\n",
    "For the face output, we have \n",
    "$$\n",
    "    \\mathbf{h}_{t+1}^2 = \\sigma \\bigg[ \\mathbf{F}_{t} \\mathbf{h}_{t}^{2}  + \\mathbf{F}_{t,\\downarrow} \\mathbf{B}_{2}^\\top  \\mathbf{h}_t^{1} \\bigg]\n",
    "$$\n",
    "where $\\mathbf{F}_t$ and $\\mathbf{F}_{t,\\downarrow}$ are two simplicial filters defined in the triangle (face) space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from toponetx import SimplicialComplex\n",
    "import toponetx.datasets as datasets\n",
    "from topomodelx.nn.simplicial.sccnn_layer import SCCNNLayer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "### Import shrec dataset ##\n",
    "\n",
    "We must first lift our graph dataset into the simplicial complex domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shrec 16 small dataset...\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "shrec, _ = datasets.mesh.shrec_16(size=\"small\")\n",
    "shrec = {key: np.array(value) for key, value in shrec.items()}\n",
    "x_0s = shrec[\"node_feat\"]\n",
    "x_1s = shrec[\"edge_feat\"]\n",
    "x_2s = shrec[\"face_feat\"]\n",
    "\n",
    "ys = shrec[\"label\"]\n",
    "simplexes = shrec[\"complexes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 10, 7)\n"
     ]
    }
   ],
   "source": [
    "in_channels_0 = x_0s[-1].shape[1]\n",
    "in_channels_1 = x_1s[-1].shape[1]\n",
    "in_channels_2 = x_2s[-1].shape[1]\n",
    "\n",
    "in_channels_all = (in_channels_0,in_channels_1,in_channels_2)\n",
    "print(in_channels_all)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neighborhood Strctures\n",
    "Get incidence matrices $\\mathbf{B}_1,\\mathbf{B}_2$ and Hodge Laplacians $\\mathbf{L}_0, \\mathbf{L}_1$ and $\\mathbf{L}_2$.\n",
    "\n",
    "Note that the original paper considered the weighted versions of these operators. However, the current TOPONETX package does not provide such feature yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rank = 2 # the order of the SC is two \n",
    "incidence_1_list = []\n",
    "incidence_2_list = []\n",
    "\n",
    "laplacian_0_list = []\n",
    "laplacian_down_1_list = []\n",
    "laplacian_up_1_list = []\n",
    "laplacian_2_list = []\n",
    " \n",
    "for simplex in simplexes: \n",
    "    incidence_1 = simplex.incidence_matrix(rank=1)\n",
    "    incidence_2 = simplex.incidence_matrix(rank=2)\n",
    "    laplacian_0 = simplex.hodge_laplacian_matrix(rank=0,weight=True)\n",
    "    laplacian_down_1 = simplex.down_laplacian_matrix(rank=1,weight=True)\n",
    "    laplacian_up_1 = simplex.up_laplacian_matrix(rank=1,weight=True)\n",
    "    laplacian_2 = simplex.hodge_laplacian_matrix(rank=2,weight=True)\n",
    "    \n",
    "    incidence_1 = torch.from_numpy(incidence_1.todense()).to_sparse()\n",
    "    incidence_2 = torch.from_numpy(incidence_2.todense()).to_sparse()\n",
    "    laplacian_0 = torch.from_numpy(laplacian_0.todense()).to_sparse()\n",
    "    laplacian_down_1 = torch.from_numpy(laplacian_down_1.todense()).to_sparse()\n",
    "    laplacian_up_1 = torch.from_numpy(laplacian_up_1.todense()).to_sparse()\n",
    "    laplacian_2 = torch.from_numpy(laplacian_2.todense()).to_sparse()\n",
    "    \n",
    "    incidence_1_list.append(incidence_1)\n",
    "    incidence_2_list.append(incidence_2)\n",
    "    laplacian_0_list.append(laplacian_0)\n",
    "    laplacian_down_1_list.append(laplacian_down_1)\n",
    "    laplacian_up_1_list.append(laplacian_up_1)\n",
    "    laplacian_2_list.append(laplacian_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the SCCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCCNN(torch.nn.Module):\n",
    "    \"\"\"SCCNN implementation for complex classification  \n",
    "    Note: In this task, we can consider the output on any order of simplices for the classification task, which of course can be amended by a readout layer.                                        \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels_all: tuple of int\n",
    "        Dimension of input features on (nodes, edges, faces)\n",
    "    intermediate_channels_all: tuple of int\n",
    "        Dimension of features of intermediate layers on (nodes, edges, faces)\n",
    "    out_channels_all: tuple of int\n",
    "        Dimension of output features on (nodes, edges, faces)\n",
    "    conv_order: int\n",
    "        Order of convolutions, we consider the same order for all convolutions \n",
    "    sc_order: int\n",
    "        SC order \n",
    "    n_layers: int\n",
    "        Numer of layers \n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels_all, \n",
    "                 intermediate_channels_all, \n",
    "                 out_channels_all, \n",
    "                 conv_order, sc_order, \n",
    "                 aggr_norm=False, update_func=None, \n",
    "                 num_classes = 1,\n",
    "                 n_layers=2):\n",
    "        super().__init__()\n",
    "        # first layer\n",
    "        # we use an MLP to map the features on simplices of different dimensions to the same dimension   \n",
    "        self.in_linear_0 = torch.nn.Linear(in_channels_all[0],intermediate_channels_all[0])\n",
    "        self.in_linear_1 = torch.nn.Linear(in_channels_all[1],intermediate_channels_all[1])\n",
    "        self.in_linear_2 = torch.nn.Linear(in_channels_all[2],intermediate_channels_all[2])\n",
    "\n",
    "        layers = []\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(\n",
    "                SCCNNLayer(in_channels=intermediate_channels_all,out_channels=out_channels_all,conv_order=conv_order,sc_order=sc_order,aggr_norm=aggr_norm,update_func=update_func)\n",
    "            )\n",
    "    \n",
    "        self.layers = torch.nn.ModuleList(layers)  \n",
    "\n",
    "        out_channels_0, out_channels_1, out_channels_2 = out_channels_all\n",
    "        self.out_linear_0 = torch.nn.Linear(out_channels_0,num_classes)\n",
    "        self.out_linear_1 = torch.nn.Linear(out_channels_1,num_classes)\n",
    "        self.out_linear_2 = torch.nn.Linear(out_channels_2,num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self,x_all,laplacian_all,incidence_all):\n",
    "        \"\"\"Forward computation. \n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        x: tuple tensors\n",
    "            (node, edge, face) features \n",
    "            each entry shape = [n_simplices, channels]\n",
    "      \n",
    "        laplacian: tuple of tensors\n",
    "            (graph laplacian L0, down edge laplacian L1_d, upper edge laplacian L1_u, face laplacian L2)\n",
    "            each entry shape = [n_simplices,n_simplices]\n",
    "\n",
    "        incidence_1: tuple of tensors\n",
    "            tuple pf order 1 and 2 incidence matrices \n",
    "            shape of B1 = [n_nodes, n_edges]\n",
    "            shape of B2 = [n_edges, n_faces]\n",
    "    \n",
    "        Returns        \n",
    "        -------\n",
    "        _ : tensor, shape = [1]\n",
    "            Label assigned to whole complex.\n",
    "        \"\"\"    \n",
    "        x_0, x_1, x_2 = x_all\n",
    "        in_x_0 = self.in_linear_0(x_0)\n",
    "        in_x_1 = self.in_linear_1(x_1)\n",
    "        in_x_2 = self.in_linear_2(x_2)\n",
    "\n",
    "        # Forward through SCCNN \n",
    "        x_all = (in_x_0,in_x_1,in_x_2)\n",
    "        for layer in self.layers:\n",
    "            x_all = layer(x_all,laplacian_all,incidence_all)\n",
    "        \n",
    "        \"\"\"\n",
    "        We pass the output on the nodes, edges and triangles to a linear layer and use the sum of the averages of outputs on each simplex for labels of complex\n",
    "        \"\"\"\n",
    "        x_0, x_1, x_2 = x_all \n",
    "\n",
    "        x_0 = self.out_linear_0(x_0)\n",
    "        x_1 = self.out_linear_1(x_1)\n",
    "        x_2 = self.out_linear_2(x_2)\n",
    " \n",
    "        # Take the average of the 2D, 1D, and 0D cell features. If they are NaN, convert them to 0.\n",
    "        two_dimensional_cells_mean = torch.nanmean(x_2, dim=0)\n",
    "        two_dimensional_cells_mean[torch.isnan(two_dimensional_cells_mean)] = 0\n",
    "        one_dimensional_cells_mean = torch.nanmean(x_1, dim=0)\n",
    "        one_dimensional_cells_mean[torch.isnan(one_dimensional_cells_mean)] = 0\n",
    "        zero_dimensional_cells_mean = torch.nanmean(x_0, dim=0)\n",
    "        zero_dimensional_cells_mean[torch.isnan(zero_dimensional_cells_mean)] = 0\n",
    "        # Return the sum of the averages\n",
    "        return  (\n",
    "            two_dimensional_cells_mean\n",
    "            + one_dimensional_cells_mean\n",
    "            + zero_dimensional_cells_mean\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model with our pre-made neighborhood structures and specify an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCCNN(\n",
      "  (in_linear_0): Linear(in_features=6, out_features=4, bias=True)\n",
      "  (in_linear_1): Linear(in_features=10, out_features=4, bias=True)\n",
      "  (in_linear_2): Linear(in_features=7, out_features=4, bias=True)\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x SCCNNLayer()\n",
      "  )\n",
      "  (out_linear_0): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (out_linear_1): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (out_linear_2): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfs/mmaosheng/.local/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "conv_order = 2\n",
    "intermediate_channels_all = (4,4,4)\n",
    "out_channels_all = intermediate_channels_all\n",
    "num_layers = 2\n",
    "\n",
    "model = SCCNN(in_channels_all=in_channels_all,intermediate_channels_all=intermediate_channels_all,out_channels_all=out_channels_all,conv_order=conv_order,sc_order=max_rank,num_classes=1,n_layers=num_layers)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "loss_fn = torch.nn.MSELoss(size_average=True,reduction='mean')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "x_0_train, x_0_test = train_test_split(x_0s, test_size=test_size, shuffle=False)\n",
    "x_1_train, x_1_test = train_test_split(x_1s, test_size=test_size, shuffle=False)\n",
    "x_2_train, x_2_test = train_test_split(x_2s, test_size=test_size, shuffle=False)\n",
    "\n",
    "incidence_1_train, incidence_1_test = train_test_split(\n",
    "    incidence_1_list, test_size=test_size, shuffle=False\n",
    ")\n",
    "incidence_2_train, incidence_2_test = train_test_split(incidence_2_list, test_size=test_size, shuffle=False)\n",
    "laplacian_0_train, laplacian_0_test = train_test_split(laplacian_0_list, test_size=test_size, shuffle=False)\n",
    "laplacian_down_1_train, laplacian_down_1_test = train_test_split(laplacian_down_1_list, test_size=test_size, shuffle=False)\n",
    "laplacian_up_1_train, laplacian_up_1_test = train_test_split(laplacian_up_1_list, test_size=test_size, shuffle=False)\n",
    "laplacian_2_train, laplacian_2_test = train_test_split(laplacian_2_list, test_size=test_size, shuffle=False)\n",
    "\n",
    "y_train, y_test = train_test_split(ys, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the SCCNN using low amount of epochs: we keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfs/mmaosheng/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/nfs/mmaosheng/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 624748.2849\n",
      "Test_loss: 831.5817\n",
      "Epoch: 2 loss: 131.9210\n",
      "Test_loss: 127.5255\n",
      "Epoch: 3 loss: 97.9960\n",
      "Test_loss: 116.0045\n",
      "Epoch: 4 loss: 90.4851\n",
      "Test_loss: 78.6304\n",
      "Epoch: 5 loss: 87.1210\n",
      "Test_loss: 50.7896\n"
     ]
    }
   ],
   "source": [
    "test_interval = 1\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    for x_0, x_1, x_2, incidence_1, incidence_2, laplacian_0, laplacian_down_1, laplacian_up_1, laplacian_2, y in zip(x_0_train, x_1_train, x_2_train, incidence_1_train, incidence_2_train, laplacian_0_train, laplacian_down_1_train, laplacian_up_1_train, laplacian_2_train, y_train):\n",
    "        \n",
    "        x_0 = torch.tensor(x_0)\n",
    "        x_1 = torch.tensor(x_1)\n",
    "        x_2 = torch.tensor(x_2)\n",
    "        y = torch.tensor(y,dtype=torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        x_all = (x_0.float(),x_1.float(),x_2.float())\n",
    "        laplacian_all = (laplacian_0, laplacian_down_1, laplacian_up_1, laplacian_2)\n",
    "        incidence_all = (incidence_1, incidence_2)\n",
    "\n",
    "        y_hat = model(x_all, laplacian_all, incidence_all)\n",
    "\n",
    "        # print(y_hat) \n",
    "        loss = loss_fn(y_hat, y)\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            for x_0, x_1, x_2, incidence_1, incidence_2, laplacian_0, laplacian_down_1, laplacian_up_1, laplacian_2, y in zip(x_0_test, x_1_test, x_2_test, incidence_1_test, incidence_2_test, laplacian_0_test, laplacian_down_1_test, laplacian_up_1_test, laplacian_2_test, y_test):\n",
    "    \n",
    "                x_0 = torch.tensor(x_0)\n",
    "                x_1 = torch.tensor(x_1)\n",
    "                x_2 = torch.tensor(x_2)\n",
    "                y = torch.tensor(y,dtype=torch.float)\n",
    "                optimizer.zero_grad()\n",
    "                x_all = (x_0.float(),x_1.float(),x_2.float())\n",
    "                laplacian_all = (laplacian_0, laplacian_down_1, laplacian_up_1, laplacian_2)\n",
    "                incidence_all = (incidence_1, incidence_2)\n",
    "\n",
    "                y_hat = model(x_all, laplacian_all, incidence_all)\n",
    "                    \n",
    "                loss = loss_fn(y_hat, y)\n",
    "            print(f\"Test_loss: {loss:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
