{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Simplicial Convolutional Neural Network (SCNN)\n",
    "\n",
    "In this notebook, we will create and train a convolutional neural network in the simplicial complex domain, as proposed in the paper by [Yang et. al : SIMPLICIAL CONVOLUTIONAL NEURAL NETWORKS (2022)](https://arxiv.org/pdf/2110.02585.pdf). \n",
    "\n",
    "We train the model to perform binary node classification using the karate club dataset.\n",
    "\n",
    "## Simplicial Convolutional Neural Networks <a href=\"https://arxiv.org/pdf/2110.02585.pdf\">[SCNN]</a>\n",
    "\n",
    "At layer $t$, given the input simplicial (edge) feature matrix $\\mathbf{H}_t$, the SCNN layer is defined as \n",
    "$$\n",
    "    \\mathbf{H}_{t+1} = \\sigma \\Bigg[ \\mathbf{H}_t\\mathbf{\\Theta}_t + \\sum_{p_d=1}^{P_d}(\\mathbf{L}_{\\downarrow,1})^{p_d}\\mathbf{H}_t\\mathbf{\\Theta}_{t,p_d} + \\sum_{p_u=1}^{P_u}(\\mathbf{L}_{\\uparrow,1})^{p_u}\\mathbf{H}_{t}\\mathbf{\\Theta}_{t,p_u} \\Bigg]\n",
    "$$\n",
    "where $p_d$ and $p_u$ are the lower and upper convolution orders, respectively, and $\\mathbf{\\Theta}_{t,p_d}$ and $\\mathbf{\\Theta}_{t,p_u}$ are the learnable weights.\n",
    "One can use $(\\mathbf{L}_{\\uparrow,1})^{p_u}$ and $(\\mathbf{L}_{\\uparrow,1})^{p_d}$ to perform higher-order upper and lower convolutions.\n",
    "\n",
    "\n",
    "To align with the notations in [Papillon et al : Architectures of Topological Deep Learning: A Survey of Topological Neural Networks (2023)](https://arxiv.org/abs/2304.10031), we can use the following to denote the above layer definition. \n",
    "\n",
    "游린 $\\quad m_{y \\rightarrow \\{z\\} \\rightarrow x}^{p_u(1 \\rightarrow 2 \\rightarrow 1)}  = ((L_{\\uparrow,1})^{p_u})_{xy} \\cdot h_y^{t,(1)} \\cdot \\theta^{t, p_u} $  -------- Aggregate from $p_u$-hop upper neighbor $y$ to $x$\n",
    "\n",
    "游린 $\\quad m_{y \\rightarrow \\{z\\} \\rightarrow x}^{p_d(1 \\rightarrow 0 \\rightarrow 1)} = ((L_{\\downarrow,1})^{p_d})_{xy} \\cdot h_y^{t,(1)} \\cdot \\theta^{t, p_d} $ -------- Aggregate from $p_d$-hop lower neighbor $y$ to $x$\n",
    "\n",
    "游린 $\\quad m^{(1 \\rightarrow 1)}_{x \\rightarrow x} = \\theta^t \\cdot h_x^{t, (1)}$ --------  Aggregate from $x$ itself\n",
    "\n",
    "游릲 $\\quad m_{x}^{p_u,(1 \\rightarrow 2 \\rightarrow 1)}  = \\sum_{y \\in \\mathcal{L}_\\uparrow(x)}m_{y \\rightarrow \\{z\\} \\rightarrow x}^{p_u,(1 \\rightarrow 2 \\rightarrow 1)}$  -------- Collect the aggregated information from the upper neighborhood\n",
    "\n",
    "游릲 $\\quad m_{x}^{p_d,(1 \\rightarrow 0 \\rightarrow 1)} = \\sum_{y \\in \\mathcal{L}_\\downarrow(x)}m_{y \\rightarrow \\{z\\} \\rightarrow x}^{p_d,(1 \\rightarrow 0 \\rightarrow 1)}$ -------- Collect the aggregated information from the lower neighborhood\n",
    "\n",
    "游릲 $\\quad m^{(1 \\rightarrow 1)}_{x} = m^{(1 \\rightarrow 1)}_{x \\rightarrow x}$\n",
    "\n",
    "游릴 $\\quad m_x^{(1)}  = m_x^{(1 \\rightarrow 1)} + \\sum_{p_u=1}^{P_u} m_{x}^{p_u,(1 \\rightarrow 2 \\rightarrow 1)} + \\sum_{p_d=1}^{P_d} m_{x}^{p_d,(1 \\rightarrow 0 \\rightarrow 1)}$ -------- Collect all the aggregated information \n",
    "\n",
    "游릱 $\\quad h_x^{t+1, (1)} = \\sigma(m_x^{(1)})$ -------- Pass through the nonlinearity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "from toponetx import SimplicialComplex\n",
    "import toponetx.datasets as datasets\n",
    "\n",
    "from topomodelx.nn.simplicial.scnn_layer import SCNNLayer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import shrec dataset ##\n",
    "\n",
    "We must first lift our graph dataset into the simplicial complex domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shrec 16 small dataset...\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "shrec, _ = datasets.mesh.shrec_16(size=\"small\")\n",
    "\n",
    "shrec = {key: np.array(value) for key, value in shrec.items()}\n",
    " \n",
    "x_0s = shrec[\"node_feat\"]\n",
    "x_1s = shrec[\"edge_feat\"]\n",
    "x_2s = shrec[\"face_feat\"]\n",
    "\n",
    "ys = shrec[\"label\"]\n",
    "simplexes = shrec[\"complexes\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider using edge features for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels_0 = x_0s[-1].shape[1]\n",
    "in_channels_1 = x_1s[-1].shape[1]\n",
    "in_channels_2 = x_2s[-1].shape[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Neighborhood Strctures\n",
    "Get incidence matrices and Hodge Laplacians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rank = 2 # the order of the SC is two \n",
    "incidence_1_list = []\n",
    "incidence_2_list = []\n",
    "\n",
    "laplacian_0_list = []\n",
    "laplacian_down_1_list = []\n",
    "laplacian_up_1_list = []\n",
    "laplacian_2_list = []\n",
    " \n",
    "for simplex in simplexes: \n",
    "    incidence_1 = simplex.incidence_matrix(rank=1)\n",
    "    incidence_2 = simplex.incidence_matrix(rank=2)\n",
    "    laplacian_0 = simplex.hodge_laplacian_matrix(rank=0,weight=True)\n",
    "    laplacian_down_1 = simplex.down_laplacian_matrix(rank=1,weight=True)\n",
    "    laplacian_up_1 = simplex.up_laplacian_matrix(rank=1,weight=True)\n",
    "    laplacian_2 = simplex.hodge_laplacian_matrix(rank=2,weight=True)\n",
    "    \n",
    "    incidence_1 = torch.from_numpy(incidence_1.todense()).to_sparse()\n",
    "    incidence_2 = torch.from_numpy(incidence_2.todense()).to_sparse()\n",
    "    laplacian_0 = torch.from_numpy(laplacian_0.todense()).to_sparse()\n",
    "    laplacian_down_1 = torch.from_numpy(laplacian_down_1.todense()).to_sparse()\n",
    "    laplacian_up_1 = torch.from_numpy(laplacian_up_1.todense()).to_sparse()\n",
    "    laplacian_2 = torch.from_numpy(laplacian_2.todense()).to_sparse()\n",
    "    \n",
    "    incidence_1_list.append(incidence_1)\n",
    "    incidence_2_list.append(incidence_2)\n",
    "    laplacian_0_list.append(laplacian_0)\n",
    "    laplacian_down_1_list.append(laplacian_down_1)\n",
    "    laplacian_up_1_list.append(laplacian_up_1)\n",
    "    laplacian_2_list.append(laplacian_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the SCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topomodelx.nn.simplicial.scnn_layer import SCNNLayer\n",
    "\n",
    "class SCNN(torch.nn.Module):\n",
    "    \"\"\"Simplicial convolutional neural network implementation for complex classification. \n",
    "    \n",
    "    Note: At the last layer, we obtain the output on simplcies, e.g., edges. \n",
    "    To perform the complex classification task for this challenge, we consider pass the final output to a linear layer and compute the average. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels: int\n",
    "        Dimension of input features \n",
    "    intermediate_channels: int\n",
    "        Dimension of features of intermediate layers\n",
    "    out_channels: int\n",
    "        Dimension of output features\n",
    "    conv_order_down: int\n",
    "        Order of lower convolution\n",
    "    conv_order_up: int\n",
    "        Order of upper convolution \n",
    "    n_layers: int\n",
    "        Numer of layers \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, intermediate_channels, out_channels,conv_order_down,conv_order_up,aggr_norm=False,update_func=None, n_layers=2):\n",
    "        super().__init__()\n",
    "        # First layer -- initial layer has the in_channels as input, and inter_channels as the output\n",
    "        layers = [SCNNLayer(in_channels=in_channels,out_channels=intermediate_channels,conv_order_down=conv_order_down,conv_order_up=conv_order_up)]\n",
    "\n",
    "        for _ in range(n_layers-1):\n",
    "            layers.append(\n",
    "                SCNNLayer(in_channels=intermediate_channels,out_channels=out_channels,conv_order_down=conv_order_down,conv_order_up=conv_order_up,aggr_norm=aggr_norm,update_func=update_func)\n",
    "            )\n",
    "            \n",
    "        self.linear = torch.nn.Linear(out_channels,1)\n",
    "        self.layers = torch.nn.ModuleList(layers)  \n",
    "\n",
    "    def forward(self, x, laplacian_down, laplacian_up):\n",
    "        \"\"\"Forward computation.\n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        x: tensor\n",
    "            shape = [n_simplices, channels]\n",
    "            node/edge/face features\n",
    "        \n",
    "        laplacian: tensor\n",
    "            shape = [n_simplices,n_simplices]\n",
    "            For node features, laplacian_down = None\n",
    "\n",
    "        incidence_1: tensor \n",
    "            order 1 incidence matrix \n",
    "            shape = [n_edges, n_nodes]\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,laplacian_down,laplacian_up)\n",
    "\n",
    "        x_1 = self.linear(x)\n",
    "        one_dimensional_cells_mean = torch.nanmean(x_1, dim=0)\n",
    "        one_dimensional_cells_mean[torch.isnan(one_dimensional_cells_mean)] = 0\n",
    "        \n",
    "        return one_dimensional_cells_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model with our pre-made neighborhood structures and specify an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 1 # simplex level \n",
    "conv_order_down = 2\n",
    "conv_order_up = 2\n",
    "intermediate_channels = 4\n",
    "out_channels = intermediate_channels\n",
    "num_layers = 2\n",
    "\n",
    "# select the simplex level\n",
    "if rank == 0: \n",
    "    laplacian_down = None\n",
    "    laplacian_up = laplacian_0_list # the graph laplacian \n",
    "    conv_order_down = 0\n",
    "    x = x_0s\n",
    "    in_channels = in_channels_0\n",
    "elif rank == 1:\n",
    "    laplacian_down = laplacian_down_1_list\n",
    "    laplacian_up = laplacian_up_1_list\n",
    "    x = x_1s\n",
    "    in_channels = in_channels_1\n",
    "elif rank == 2:\n",
    "    laplacian_down = laplacian_2_list\n",
    "    laplacian_up = None \n",
    "    x = x_2s\n",
    "    in_channels = in_channels_2 \n",
    "else: \n",
    "    raise ValueError(\n",
    "        f\"Rank must be not larger than 2 on this dataset\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SCNN(in_channels = in_channels,intermediate_channels = intermediate_channels,out_channels = out_channels, conv_order_down=conv_order_down,conv_order_up=conv_order_up,n_layers=num_layers\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "x_train, x_test = train_test_split(x, test_size=test_size, shuffle=False)\n",
    "\n",
    "laplacian_down_train, laplacian_down_test = train_test_split(laplacian_down, test_size=test_size, shuffle=False)\n",
    "laplacian_up_train, laplacian_up_test = train_test_split(laplacian_up, test_size=test_size, shuffle=False)\n",
    "y_train, y_test = train_test_split(ys, test_size=test_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfs/mmaosheng/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/nfs/mmaosheng/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 261.3912\n",
      "Test_loss: 97.1429\n",
      "Epoch: 2 loss: 94.8871\n",
      "Test_loss: 120.7043\n",
      "Epoch: 3 loss: 83.3380\n",
      "Test_loss: 122.5341\n",
      "Epoch: 4 loss: 83.5518\n",
      "Test_loss: 120.2451\n",
      "Epoch: 5 loss: 85.0916\n",
      "Test_loss: 110.6804\n"
     ]
    }
   ],
   "source": [
    "test_interval = 1\n",
    "num_epochs = 5\n",
    "\n",
    "# select which feature to use for labeling\n",
    "simplex_order_select = 1\n",
    "\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    for x, laplacian_down, laplacian_up, y in zip(x_train, laplacian_down_train, laplacian_up_train, y_train):\n",
    "\n",
    "        x = torch.tensor(x,dtype=torch.float)\n",
    "        y = torch.tensor(y,dtype=torch.float)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_hat = model(x, laplacian_down, laplacian_up)\n",
    "\n",
    "        # print(y_hat.shape)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            for x, laplacian_down, laplacian_up, y in zip(x_test, laplacian_down_test, laplacian_up_test, y_test):\n",
    "    \n",
    "                x = torch.tensor(x,dtype=torch.float)\n",
    "                y = torch.tensor(y,dtype=torch.float)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_hat = model(x, laplacian_down, laplacian_up)\n",
    "\n",
    "                    \n",
    "                loss = loss_fn(y_hat, y)\n",
    "            print(f\"Test_loss: {loss:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
