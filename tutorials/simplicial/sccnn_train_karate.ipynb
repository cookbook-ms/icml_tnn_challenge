{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a SCCNN\n",
    "\n",
    "In this notebook, we will create and train a High Skip Network in the simplicial complex domain, as proposed in the paper by [Yang et. al : Convolutional Learning on Simplicial Complexes (2023)](https://arxiv.org/abs/2301.11163). \n",
    "\n",
    "We train the model to perform binary node classification using the KarateClub benchmark dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from toponetx import SimplicialComplex\n",
    "import toponetx.datasets.graph as graph\n",
    "\n",
    "from topomodelx.nn.simplicial.sccnn_layer import SCCNNLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "## Import dataset ##\n",
    "\n",
    "The first step is to import the Karate Club (https://www.jstor.org/stable/3629752) dataset. This is a singular graph with 34 nodes that belong to two different social groups. We will use these groups for the task of node-level binary classification.\n",
    "\n",
    "We must first lift our graph dataset into the simplicial complex domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplicial Complex with shape (34, 78, 45, 11, 2) and dimension 4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "dataset = graph.karate_club(complex_type=\"simplicial\")\n",
    "print(dataset)\n",
    "max_rank = dataset.dim\n",
    "print(max_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The incidence matrix B1 has shape: (34, 78).\n",
      "The incidence matrix B2 has shape: (78, 45).\n"
     ]
    }
   ],
   "source": [
    "incidence_1 = dataset.incidence_matrix(rank=1)\n",
    "incidence_2 = dataset.incidence_matrix(rank=2)\n",
    "\n",
    "print(f\"The incidence matrix B1 has shape: {incidence_1.shape}.\")\n",
    "print(f\"The incidence matrix B2 has shape: {incidence_2.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian_0  = dataset.hodge_laplacian_matrix(rank=0,weight=True)\n",
    "laplacian_down_1 = dataset.down_laplacian_matrix(rank=1,weight=True)\n",
    "laplacian_up_1 = dataset.up_laplacian_matrix(rank=1,weight=True)\n",
    "laplacian_down_2 = dataset.down_laplacian_matrix(rank=2,weight=True)\n",
    "laplacian_up_2 = dataset.up_laplacian_matrix(rank=2,weight=True)\n",
    "\n",
    "laplacian_0  = dataset.adjacency_matrix(rank=0,weight=True)\n",
    "laplacian_down_1 = dataset.coadjacency_matrix(rank=1,weight=True)\n",
    "laplacian_up_1 = dataset.adjacency_matrix(rank=1,weight=True)\n",
    "laplacian_down_2 = dataset.coadjacency_matrix(rank=2,weight=True)\n",
    "laplacian_up_2 = dataset.adjacency_matrix(rank=2,weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spl\n",
    "\n",
    "# def normalize(L, half_interval = False):\n",
    "#     assert(sp.isspmatrix(L))\n",
    "#     M = L.shape[0]\n",
    "#     assert(M == L.shape[1])\n",
    "#     topeig = spl.eigsh(L, k=1, which=\"LM\", return_eigenvectors = False)[0]   \n",
    "#     #print(\"Topeig = %f\" %(topeig))\n",
    "\n",
    "#     ret = L.copy()\n",
    "#     if half_interval:\n",
    "#         ret *= 1.0/topeig\n",
    "#     else:\n",
    "#         ret *= 2.0/topeig\n",
    "#         ret.setdiag(ret.diagonal(0) - np.ones(M), 0)\n",
    "\n",
    "#     return ret\n",
    "\n",
    "# laplacian_0 = normalize(laplacian_0)\n",
    "# laplacian_down_1 = normalize(laplacian_down_1)\n",
    "# laplacian_up_1 = normalize(laplacian_up_1)\n",
    "# laplacian_down_2 = normalize(laplacian_down_2)\n",
    "# laplacian_up_2 = normalize(laplacian_up_2)\n",
    "\n",
    "# def normalize_incidence(B):\n",
    "#     row_sums = B.sum(axis=1)\n",
    "#     B = B/row_sums[:,np.newaxis]\n",
    "\n",
    "# def normalizeRows(x):\n",
    "#     \"\"\"\n",
    "#     Implement a function that normalizes each row of the matrix x (to have unit length).\n",
    "    \n",
    "#     Argument:\n",
    "#     x -- A numpy matrix of shape (n, m)\n",
    "    \n",
    "#     Returns:\n",
    "#     x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n",
    "#     x_norm = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "#     print(\"x_norm.shape:\", x_norm.shape, \"\\n\")\n",
    "    \n",
    "#     # Divide x by its norm.\n",
    "#     x = x / x_norm\n",
    "\n",
    "#     return x    \n",
    "\n",
    "# incidence_1 = sp.csr_matrix(normalizeRows(incidence_1.toarray()))\n",
    "# incidence_2 = sp.csr_matrix(normalizeRows(incidence_2.toarray()))\n",
    "# print(incidence_1)\n",
    "\n",
    "\n",
    "# # from scipy.sparse.linalg import eigsh\n",
    "# # from scipy import sparse \n",
    "\n",
    "# # eig_0, _ = eigsh(laplacian_0,k=1, which='LM')\n",
    "# # laplacian_0 = sparse.csr_matrix(laplacian_0/eig_0)\n",
    "# # eig_down_1, _ = eigsh(laplacian_down_1,k=1,which='LM')\n",
    "# # laplacian_down_1 = sparse.csr_matrix(laplacian_down_1/eig_down_1)\n",
    "# # eig_up_1, _ = eigsh(laplacian_up_1,k=1,which='LM')\n",
    "# # laplacian_up_1 = sparse.csr_matrix(laplacian_up_1/eig_up_1)\n",
    "# # eig_down_2, _ = eigsh(laplacian_down_2,k=1,which='LM')\n",
    "# # laplacian_down_2 = sparse.csr_matrix(laplacian_down_2/eig_down_2)\n",
    "# # eig_up_2, _ = eigsh(laplacian_up_2,k=1,which='LM')\n",
    "# # laplacian_up_2 = sparse.csr_matrix(laplacian_up_2/eig_up_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "laplacian_0 = torch.from_numpy(laplacian_0.todense()).to_sparse()\n",
    "laplacian_down_1 = torch.from_numpy(laplacian_down_1.todense()).to_sparse()\n",
    "laplacian_up_1 = torch.from_numpy(laplacian_up_1.todense()).to_sparse()\n",
    "laplacian_down_2 = torch.from_numpy(laplacian_down_2.todense()).to_sparse()\n",
    "laplacian_up_2 = torch.from_numpy(laplacian_up_2.todense()).to_sparse()\n",
    "\n",
    "incidence_1 = torch.from_numpy(incidence_1.todense()).to_sparse()\n",
    "incidence_2 = torch.from_numpy(incidence_2.todense()).to_sparse()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import signal ##\n",
    "\n",
    "Since our task will be node classification, we must retrieve an input signal on the nodes. The signal will have shape $n_\\text{nodes} \\times$ in_channels, where in_channels is the dimension of each cell's feature. Here, we have in_channels = channels_nodes $ = 34$. This is because the Karate dataset encodes the identity of each of the 34 nodes as a one hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A function to obtain features based on the input: rank\n",
    "\"\"\"\n",
    "def get_simplicial_features(dataset,rank):\n",
    "    if rank == 0: \n",
    "        which_feat = \"node_feat\"\n",
    "    elif rank == 1:\n",
    "        which_feat = \"edge_feat\"\n",
    "    elif rank == 2:\n",
    "        which_feat = \"face_feat\"\n",
    "    else:\n",
    "        raise ValueError(f\"input dimension must be 0, 1 or 2, because features are supported on nodes, edges and faces\") \n",
    "    \n",
    "    x = []\n",
    "    for _, v in dataset.get_simplex_attributes(which_feat).items():\n",
    "        x.append(v)\n",
    "    \n",
    "    x = torch.tensor(np.stack(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define binary labels\n",
    "We retrieve the labels associated to the nodes of each input simplex. In the KarateClub dataset, two social groups emerge. So we assign binary labels to the nodes indicating of which group they are a part.\n",
    "\n",
    "We convert the binary labels into one-hot encoder form, and keep the first four nodes' true labels for the purpose of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(\n",
    "    [\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        1,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "    ]\n",
    ")\n",
    "y_true = np.zeros((34, 2))\n",
    "y_true[:, 0] = y\n",
    "y_true[:, 1] = 1 - y\n",
    "y_test = y_true[-4:]\n",
    "y_train = y_true[:30]\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the SCCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCCNN(torch.nn.Module):\n",
    "    \"\"\"SCCNN implementation for binary node classification \n",
    "    Note: In this task, we direcly consider the finaly output on the nodes, which is passed by a linear layer, as the label output. \n",
    "\n",
    "    Parameters\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels_all, intermediate_channels_all,out_channels_all, conv_order, sc_order, aggr_norm=True,update_func=\"sigmoid\",n_layers=2):\n",
    "        super().__init__()\n",
    "        # first layer \n",
    "        layers = [SCCNNLayer(in_channels=in_channels_all,out_channels=intermediate_channels_all,conv_order=conv_order,sc_order=sc_order,aggr_norm=aggr_norm,update_func=update_func)]\n",
    "\n",
    "        for _ in range(n_layers-1):\n",
    "            layers.append(\n",
    "                SCCNNLayer(in_channels=intermediate_channels_all,out_channels=out_channels_all,conv_order=conv_order,sc_order=sc_order,aggr_norm=aggr_norm,update_func=update_func)\n",
    "            )\n",
    "        out_channels_0, out_channels_1, out_channels_2 = out_channels_all\n",
    "        self.linear = torch.nn.Linear(out_channels_0,2)\n",
    "        self.layers = layers \n",
    "\n",
    "    def forward(self,x_all,laplacian_all,incidence_all):\n",
    "        \"\"\"Forward computation. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x_all = layer(x_all,laplacian_all,incidence_all)\n",
    "        \n",
    "        \"\"\"\n",
    "        We pass the output on the ndoes to a linear layers and use that for labels\n",
    "        \"\"\"\n",
    "        x_0, _, _ = x_all \n",
    "        logits = self.linear(x_0)\n",
    "        label = torch.sigmoid(logits)\n",
    "        \n",
    "        return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We specify the model with our pre-made neighborhood structures and specify an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Obtain the initial features on all simplices\"\"\"\n",
    "x_0 = get_simplicial_features(dataset,rank=0)\n",
    "x_1 = get_simplicial_features(dataset,rank=1)\n",
    "x_2 = get_simplicial_features(dataset,rank=2)\n",
    "\n",
    "x_all = (x_0,x_1,x_2)\n",
    "\n",
    "conv_order = 5\n",
    "in_channels_all = (x_0.shape[-1],x_1.shape[-1],x_2.shape[-1])\n",
    "intermediate_channels_all = (4,4,4)\n",
    "out_channels_all = intermediate_channels_all\n",
    "num_layers = 2\n",
    "\n",
    "laplacian_all = (laplacian_0,laplacian_down_1,laplacian_up_1,laplacian_down_2,laplacian_up_2)\n",
    "\n",
    "incidence_all = (incidence_1,incidence_2)\n",
    "\n",
    "model = SCCNN(in_channels_all=in_channels_all,intermediate_channels_all=intermediate_channels_all,out_channels_all=out_channels_all,conv_order=conv_order,sc_order=max_rank,n_layers=num_layers)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 0.8414 Train_acc: 0.4333\n",
      "Test_acc: 1.0000\n",
      "Epoch: 2 loss: 0.8338 Train_acc: 0.4333\n",
      "Test_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "test_interval = 1\n",
    "num_epochs = 2\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    epoch_loss = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_hat = model(x_all, laplacian_all, incidence_all)\n",
    "    loss = torch.nn.functional.binary_cross_entropy(\n",
    "        y_hat[: len(y_train)].float(), y_train.float()\n",
    "    )\n",
    "    epoch_loss.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.where(y_hat > 0.5, torch.tensor(1), torch.tensor(0))\n",
    "    accuracy = (y_pred[-len(y_train) :] == y_train).all(dim=1).float().mean().item()\n",
    "    print(\n",
    "        f\"Epoch: {epoch_i} loss: {np.mean(epoch_loss):.4f} Train_acc: {accuracy:.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    if epoch_i % test_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            y_hat_test = model(x_all, laplacian_all, incidence_all)\n",
    "            y_pred_test = torch.where(\n",
    "                y_hat_test > 0.5, torch.tensor(1), torch.tensor(0)\n",
    "            )\n",
    "            test_accuracy = (\n",
    "                torch.eq(y_pred_test[-len(y_test) :], y_test)\n",
    "                .all(dim=1)\n",
    "                .float()\n",
    "                .mean()\n",
    "                .item()\n",
    "            )\n",
    "            print(f\"Test_acc: {test_accuracy:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
